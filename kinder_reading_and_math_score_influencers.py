# -*- coding: utf-8 -*-
"""Kinder Reading and Math Score Influencers

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1qzporeOUX-bdnhbIgpWp7vJwFSyUPJS6

Influences Of Kindergarteners' Reading and Math Scores

## EDA
"""

# This project attempts to explore the drivers of read and math scores across multiple variables
# Runs a variety of regression models (OLS, random forest, LASSO/Ridge)
# Returns the most appropriate model (lowest MSE score).

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

from sklearn.model_selection import train_test_split, GridSearchCV
from sklearn.linear_model import LinearRegression, RidgeCV, LassoCV
from sklearn.ensemble import RandomForestRegressor
from sklearn.impute import SimpleImputer
from sklearn.metrics import mean_squared_error

# plotting style
sns.set(style="whitegrid", palette="muted")

# load data
sample_url = "https://drive.google.com/uc?export=download&id=1ekCjJOTjGD0OASErwVnwhJuPD0hFfCEF"
out_url = sample_url

base_df = pd.read_csv(sample_url)
db_out = pd.read_csv(out_url)

print("Data shape:", base_df.shape)
base_df.head()

import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

sample_kinder_data = base_df.copy()


sns.set_theme(style="whitegrid")


plt.figure(figsize=(8, 6))
sns.histplot(data=sample_kinder_data, x="score_read", binwidth=20, color="red", edgecolor="white", alpha=0.8)

plt.title("Distribution of Reading Scores")
plt.xlabel("Reading Score")
plt.ylabel("Frequency")
plt.tight_layout()
plt.show()

import matplotlib.pyplot as plt
import seaborn as sns


plt.figure(figsize=(8, 6))
sns.histplot(data=sample_kinder_data, x="score_math", binwidth=20, color="#F28E2B", edgecolor="white", alpha=0.8)

plt.title("Distribution of Math Scores")
plt.xlabel("Math Score")
plt.ylabel("Frequency")
sns.set_theme(style="whitegrid")
plt.tight_layout()
plt.show()

# Correlation analysis (numeric + dummy variables)

# Start from original df that still has score_read and score_math
df_corr = base_df.copy()

# encode dummy variables
df_corr = pd.get_dummies(df_corr, drop_first=False)

print("Shape after dummies for correlation:", df_corr.shape)

# Full correlation matrix
corr = df_corr.corr()

#  A. Correlation with outcomes

# All variables' correlations with score_read and score_math
target_corr = corr[["score_read", "score_math"]].drop(["score_read", "score_math"], errors="ignore")

# Sort by |corr with reading|
target_corr_sorted = target_corr.reindex(
    target_corr["score_read"].abs().sort_values(ascending=False).index
)

print("\nTop 15 variables most correlated with Reading score:")
display(
    target_corr_sorted.head(15)
    .style.background_gradient(cmap="coolwarm", axis=None, vmin=-1, vmax=1)
    .format(precision=2)
)

print("\nTop 15 variables most correlated with Math score:")
display(
    target_corr_sorted.sort_values(by="score_math", key=abs, ascending=False)
    .head(15)
    .style.background_gradient(cmap="coolwarm", axis=None, vmin=-1, vmax=1)
    .format(precision=2)
)

#  B. Bar plots for top correlations with outcomes

top15_read = target_corr_sorted["score_read"].head(15).sort_values()
top15_math = target_corr_sorted["score_math"].head(15).sort_values()

plt.figure(figsize=(7, 8))
plt.barh(top15_read.index, top15_read.values, color="steelblue")
plt.title("Top 15 Correlations with Reading Score", fontsize=13)
plt.xlabel("Pearson correlation (r)")
for i, v in enumerate(top15_read.values):
    plt.text(v + 0.01 * np.sign(v), i, f"{v:.2f}", va='center',
             ha='left' if v >= 0 else 'right')
plt.tight_layout()
plt.show()

plt.figure(figsize=(7, 8))
plt.barh(top15_math.index, top15_math.values, color="indianred")
plt.title("Top 15 Correlations with Math Score", fontsize=13)
plt.xlabel("Pearson correlation (r)")
for i, v in enumerate(top15_math.values):
    plt.text(v + 0.01 * np.sign(v), i, f"{v:.2f}", va='center',
             ha='left' if v >= 0 else 'right')
plt.tight_layout()
plt.show()

#  C. Full correlation heat maps

cols = corr.columns.tolist()
n_cols = len(cols)
print(f"\nTotal variables in correlation matrix: {n_cols}")

# Column groups: 1–10, 11–20, 21–30, 31–40, 40-end
col_groups = [cols[i:i+10] for i in range(0, 40, 10)]
col_groups.append(cols[40:])

for idx, col_subset in enumerate(col_groups, start=1):
    sub_corr = corr.loc[:, col_subset]   # all rows, subset of columns
    plt.figure(figsize=(10, 14))
    sns.heatmap(
        sub_corr,
        annot=True,
        fmt=".2f",
        cmap="coolwarm",
        center=0,
        linewidths=0.4,
        cbar_kws={'label': 'Pearson r'}
    )
    plt.title(
        f"Correlation Heatmap – Columns {col_subset[0]} … {col_subset[-1]} "
        f"(Block {idx})",
        fontsize=13,
        pad=18
    )
    plt.xticks(rotation=90)
    plt.yticks(rotation=0)
    plt.tight_layout()
    plt.show()

# Identify correlated pairs ( high: |corr| > 0.8 moderate: |corr| 0.5-0.8)
corr_pairs = corr.abs().unstack().sort_values(ascending=False)
high_corr = corr_pairs[(corr_pairs < 1.0) & (corr_pairs > 0.8)]
print("\nHighly correlated feature pairs (>0.8):")
display(high_corr)
mod_corr = corr_pairs[(corr_pairs <= 0.8) & (corr_pairs >= 0.5)]
print("\nModerately correlated feature pairs (0.5-0.8):")
display(mod_corr)

print(base_df['ethnicity'].value_counts())

# Cross-tab between school type and ethnicity
cross_tab = pd.crosstab(base_df['school'], base_df['ethnicity'], normalize='index') * 100
display(cross_tab.round(1))

# Stacked bar plot of ethnicity composition by school type
cross_tab.plot(
    kind='bar',
    stacked=True,
    figsize=(8,6),
    colormap='coolwarm',
    edgecolor='black'
)
plt.title("Ethnicity Composition by School Type (%)")
plt.ylabel("Percentage of Students")
plt.xlabel("School Type")
plt.legend(title="Ethnicity", bbox_to_anchor=(1.05, 1), loc='upper left')
plt.tight_layout()
plt.show()

"""## Random Forest Model - Gus Whelan

1. Create separate dataframes for maths and reading predictions
"""

import pandas as pd
import numpy as np
from sklearn.impute import SimpleImputer

# copy sample dataframe for use in RF model
df_RF = base_df.copy()

# correlated pair (school_id, schooldistrict_id)
df_RF["birth"] = df_RF["birth"].str.extract(r"(\d{4})").astype(float)
df_RF["birth_missing"] = df_RF["birth"].isna().astype(int)
df_RF["birth"] = df_RF["birth"].fillna(df_RF["birth"].median()).astype(int)
df_RF = df_RF.drop(columns=["schooldistrict_id"])
df_read = df_RF.drop(columns=["score_math"]).dropna(subset=["score_read"])
df_math = df_RF.drop(columns=["score_read"]).dropna(subset=["score_math"])

"""---
schooldistrict_id and school_id were found to be highly correlated so schooldistrict_id was dropped. Though Random Forest is capable of dealing with multicolinearity, it was decided to drop one of the pair to marginally reduce computational cost


---

2. Create score vectors and respective feature matrices and split into train/val and test datasets
"""

from sklearn.model_selection import train_test_split

# score vectors
y_read = df_read["score_read"]
y_math = df_math["score_math"]

# feature matrices
X_read = df_read.drop(columns=["score_read"])
X_math = df_math.drop(columns=["score_math"])

# get dummies for cat variables
X_read = pd.get_dummies(X_read, drop_first=True)
X_math = pd.get_dummies(X_math, drop_first=True)

# impute predictors with mean
imputer = SimpleImputer(strategy='mean')
X_read_imputed = pd.DataFrame(imputer.fit_transform(X_read), columns=X_read.columns)
X_math_imputed = pd.DataFrame(imputer.fit_transform(X_math), columns=X_math.columns)

print("X_read_imputed shape:", X_read_imputed.shape)
print("y_read shape", y_read.shape)

print("X_math_imputed shape:", X_math_imputed.shape)
print("y_math shape", y_math.shape)

# create train/val and test subsets on 80/20 split
X_read_trainval, X_read_test, y_read_trainval, y_read_test = train_test_split(
    X_read_imputed, y_read, test_size=0.2, random_state=42
)

print("\nX_read_trainval shape:", X_read_trainval.shape)
print("y_read_trainval shape", y_read_trainval.shape)
print("X_read_test shape:", X_read_test.shape)
print("y_read_test shape", y_read_test.shape)

X_math_trainval, X_math_test, y_math_trainval, y_math_test = train_test_split(
    X_math_imputed, y_math, test_size=0.2, random_state=42
)

print("\nX_math_trainval shape:", X_math_trainval.shape)
print("y_math_trainval shape", y_math_trainval.shape)
print("X_math_test shape:", X_math_test.shape)
print("y_math_test shape", y_math_test.shape)

"""---
Some observations had only reading score and vice versa, hence the difference in shape between read and math dataframes. Dropping these rows would unnecessarily reduce sample size and could bias the analysis.

Instead, 2 datasets were constructed:


*   df_read: retains all obs with a valid reading score
*   df_math: retains all obs with a valid maths score

The Random Forest model will estimate reading and maths scores separately using the respective dataset to ensure maximal sample usage and avoid deleting valid outcome information.

Additionally, a random 80/20 split for training/validation and testing was performed on each dataset to enable a true out-of-sample test once hyperparameter tuning was complete

---

3. Define Random Forest Class
adapted from youtube video on Random Forest Regressor class

AssemblyAI. (2022) How to implement Random Forest from scratch with Python [online video]. YouTube, 16 Sept 2022. Available at: https://www.youtube.com/watch?v=kFwe2ZZU7yw (Accessed: 8 November 2025).
"""

from sklearn import tree
from sklearn.tree import DecisionTreeRegressor as DTR

class RandomForest:
    """
    Random Forest implementation using:
    - Bootstrap sampling
    - Random feature subsets
    - DecisionTreeRegressor as base learner
    """
    def __init__(
        self,
        n_estimators=100,
        max_depth=None,
        min_samples_split=2,
        min_samples_leaf=1,
        max_features="sqrt",
        random_state=42
    ):
        self.n_estimators = n_estimators
        self.max_depth = max_depth
        self.min_samples_split = min_samples_split
        self.min_samples_leaf = min_samples_leaf
        self.max_features = max_features
        self.random_state = random_state

        self.trees = []
        self.feature_subsets = []
        self.n_features = None

        # set global RNG seed - reproducible feature subsampling
        np.random.seed(random_state)

    # Choose random subset of features for one tree
    def _subsample_features(self, n_features):
        if self.max_features == "sqrt":
            k = int(np.sqrt(n_features))
        else:
            k = int(self.max_features * n_features)

        return np.random.choice(n_features, k, replace=False)

    # Draw bootstrap sample from X and y
    def _bootstrap_sample(self, X, y):
        n = len(X)
        idx = np.random.choice(n, n, replace=True)
        return X.iloc[idx], y.iloc[idx]

    # fit all trees in forest
    def fit(self, X, y):
        self.n_features = X.shape[1]
        self.trees = []
        self.feature_subsets = []

        for _ in range(self.n_estimators):
            # bootstrap sample of rows
            X_s, y_s = self._bootstrap_sample(X, y)

            # random subset of columns
            feat_idx = self._subsample_features(self.n_features)
            self.feature_subsets.append(feat_idx)

            # train decision tree on subset
            tree = DTR(
                max_depth=self.max_depth,
                min_samples_split=self.min_samples_split,
                min_samples_leaf=self.min_samples_leaf
            )
            tree.fit(X_s.iloc[:, feat_idx], y_s)
            self.trees.append(tree)

    # predict by averaging predictions from all trees
    def predict(self, X):
        preds = np.zeros((len(X), self.n_estimators))
        for i, (tree, feat_idx) in enumerate(zip(self.trees, self.feature_subsets)):
            preds[:, i] = tree.predict(X.iloc[:, feat_idx])
        return preds.mean(axis=1)

"""---
we decided to compute feature importance using permutation importance instead of an impurity-based approach as this can be misleading for features with high cardinality like school_id. This is performed at the end of the implementation.

---

4. Define tuning function - grid search with 5-fold CV
"""

from sklearn.model_selection import KFold
from sklearn.metrics import mean_squared_error as MSE
import itertools

# hyperparameter tuning for  RF using K-Fold CV
# returns best_params dict and a DF of all results
def tune_rf(X, y, param_grid, n_splits=5, random_state=42):
    kf = KFold(n_splits=n_splits, shuffle=True, random_state=random_state)
    results = []

    keys = list(param_grid.keys())

    for values in itertools.product(*param_grid.values()):
        params = dict(zip(keys, values))
        cv_mses = []

        for train_idx, val_idx in kf.split(X):
            X_train, X_val = X.iloc[train_idx], X.iloc[val_idx]
            y_train, y_val = y.iloc[train_idx], y.iloc[val_idx]

            model = RandomForest(**params, random_state=None)
            model.fit(X_train, y_train)
            preds = model.predict(X_val)
            mse = MSE(y_val, preds)
            cv_mses.append(mse)

        mean_mse = np.mean(cv_mses)
        std_mse = np.std(cv_mses)

        results.append({
            **params,
            "mean_mse": mean_mse,
            "std_mse": std_mse
        })

    results_df = pd.DataFrame(results).sort_values("mean_mse").reset_index(drop=True)
    best_params = results_df.iloc[0][keys].to_dict()
    return best_params, results_df

"""Define a parameter grid"""

param_grid = {
    'n_estimators': [100, 200, 300],
    'max_depth': [5, 10, 15],
    'min_samples_split': [5, 8, 10],
    'min_samples_leaf': [1, 2, 4],
    "max_features": ["sqrt", 0.25, 0.5]
}

"""5. Tune for maths and reading
---
Note for marker: this cell takes 1 hour to run so I have changed it to a markdown cell for the implementation and carry forward the pre-computed best parameters to streamline the notebook.

from pandas.core.indexing import convert_from_missing_indexer_tuple

\# Reading

best_params_read, cv_results_read = tune_rf(X_read_trainval, y_read_trainval, param_grid)

print("best parameters(Reading):")

print(best_params_read)

print("\nTop 5 configurations (Reading):")

display(cv_results_read.head())

\# Maths

best_params_math, cv_results_math = tune_rf(X_math_trainval, y_math_trainval, param_grid)

print("\n\nbest parameters(Maths):")

print(best_params_math)

print("\nTop 5 configurations (Maths):")

display(cv_results_math.head())

The following cell contains the output from the previous cell without having to re-run during marking
"""

data_reading = {
    "n_estimators": [300, 100, 300, 100, 300],
    "max_depth": [15, 15, 15, 15, 15],
    "min_samples_split": [10, 10, 8, 10, 10],
    "min_samples_leaf": [1, 1, 2, 2, 2],
    "max_features": [0.5, 0.5, 0.5, 0.5, 0.5],
    "mean_mse": [744.089056, 745.179047, 746.468051, 746.710823, 747.066846],
    "std_mse": [40.610471, 36.993780, 37.364225, 37.280632, 37.630313]
}

data_maths = {
    "n_estimators": [300, 300, 300, 300, 200],
    "max_depth": [15, 15, 15, 15, 15],
    "min_samples_split": [5, 8, 5, 8, 10],
    "min_samples_leaf": [1, 1, 2, 2, 1],
    "max_features": [0.5, 0.5, 0.5, 0.5, 0.5],
    "mean_mse": [1666.364510, 1667.636908, 1667.841680, 1667.916316, 1668.003272],
    "std_mse": [67.866281, 67.769681, 68.014069, 70.730633, 68.457939]
}

best_params_read = {
    "n_estimators": 300,
    "max_depth": 15,
    "min_samples_split": 10,
    "min_samples_leaf": 1,
    "max_features": 0.5
}

best_params_math = {
    "n_estimators": 300,
    "max_depth": 15,
    "min_samples_split": 5,
    "min_samples_leaf": 1,
    "max_features": 0.5
}

df_best_params_read = pd.DataFrame(data_reading)
print("\n\nbest parameters(Reading):")
print(best_params_read)
print("\nTop 5 configurations (Reading):")
display(df_best_params_read.head())

df_best_params_math = pd.DataFrame(data_maths)
print("\n\nbest parameters(Maths):")
print(best_params_math)
print("\nTop 5 configurations (Maths):")
display(df_best_params_math.head())

"""6. Define repeated K-Fold evaluation using best parameters
---
tests stability across multiple splits using the best parameters found previously
"""

from sklearn.model_selection import RepeatedKFold as RKF


# Evaluate the tuned RF model using repeated K-Fold CV.
# Returns mean MSE, std of MSE, and list of all MSEs

def eval_rf(X, y, best_params, n_splits=5, n_repeats=10, random_state=99):
    rkf = RKF(n_splits=n_splits, n_repeats=n_repeats, random_state=random_state)
    mses = []

    for train_idx, val_idx in rkf.split(X):
        X_train, X_val = X.iloc[train_idx], X.iloc[val_idx]
        y_train, y_val = y.iloc[train_idx], y.iloc[val_idx]

        model = RandomForest(**best_params, random_state=None)
        model.fit(X_train, y_train)
        preds = model.predict(X_val)
        mse = MSE(y_val, preds)
        mses.append(mse)

    return np.mean(mses), np.std(mses), mses

"""7. Call repeated K-Fold function for reading and maths"""

#reading
RF_mean_mse_read_rkf, RF_std_mse_read_rkf, RF_mses_read_rkf = eval_rf(
    X_read_trainval,
    y_read_trainval,
    best_params_read,
    n_splits=5,
    n_repeats=10,
    random_state=99
)

print("Random Forest (Reading) – Repeated 10x5 K-Fold on trainval")
print(f"Mean MSE (trainval RKF): {RF_mean_mse_read_rkf:.3f}")
print(f"Std MSE  (trainval RKF): {RF_std_mse_read_rkf:.3f}")

# maths
RF_mean_mse_math_rkf, RF_std_mse_math_rkf, RF_mses_math_rkf = eval_rf(
    X_math_trainval,
    y_math_trainval,
    best_params_math,
    n_splits=5,
    n_repeats=10,
    random_state=99
)

print("\nRandom Forest (Maths) – Repeated 10x5 K-Fold on trainval")
print(f"Mean MSE (trainval RKF): {RF_mean_mse_math_rkf:.3f}")
print(f"Std MSE  (trainval RKF): {RF_std_mse_math_rkf:.3f}")

"""8. Final refit on trainval + evaluation on untouched 20% test dataset"""

# Reading
final_rf_read = RandomForest(**best_params_read, random_state=42)
final_rf_read.fit(X_read_trainval, y_read_trainval)

preds_read_test = final_rf_read.predict(X_read_test)
mse_read_test = MSE(y_read_test, preds_read_test)

print("Final test evaluation (Reading):")
print(f"test MSE: {mse_read_test:.3f}")

# Maths
final_rf_math = RandomForest(**best_params_math, random_state=42)
final_rf_math.fit(X_math_trainval, y_math_trainval)

preds_math_test = final_rf_math.predict(X_math_test)
mse_math_test = MSE(y_math_test, preds_math_test)

print("Final test evaluation (Maths):")
print(f"test MSE: {mse_math_test:.3f}")

combined_mse = 0.5 * (mse_read_test + mse_math_test)
print(f"Combined final test mse: {combined_mse:.3f}")

"""9. Feature importance using permutation importance
Code adapted from sci-kit learn manual
https://scikit-learn.org/stable/auto_examples/ensemble/plot_forest_importances.html
"""

from sklearn.inspection import permutation_importance
import matplotlib.pyplot as plt

# reading
result_read = permutation_importance(
    final_rf_read,
    X_read_trainval,
    y_read_trainval,
    n_repeats=10,
    random_state=42,
    n_jobs=-1,
    scoring="neg_mean_squared_error"
)

imp_read = pd.Series(
    result_read.importances_mean,
    index=X_read_trainval.columns
).sort_values(ascending=False)

top15_read = imp_read.head(15)

# displayed in dataframe
df_top15_read = top15_read.reset_index()
df_top15_read.columns = ["feature", "importance"]

print("\nTOP 15 Predictors – READING (Permutation Importance)")
display(df_top15_read)

# displayed as horizontal bar plot
plt.figure(figsize=(8,6))
plt.barh(df_top15_read["feature"][::-1], df_top15_read["importance"][::-1])
plt.title("Top 15 Predictors – Reading (Permutation Importance)")
plt.xlabel("Importance")
plt.tight_layout()
plt.show()



# maths
result_math = permutation_importance(
    final_rf_math,
    X_math_trainval,
    y_math_trainval,
    n_repeats=10,
    random_state=42,
    n_jobs=-1,
    scoring="neg_mean_squared_error"
)

imp_math = pd.Series(
    result_math.importances_mean,
    index=X_math_trainval.columns
).sort_values(ascending=False)

top15_math = imp_math.head(15)

# displayed as dataframe
df_top15_math = top15_math.reset_index()
df_top15_math.columns = ["feature", "importance"]

print("\nTOP 15 Predictors – MATHS (Permutation Importance)")
display(df_top15_math)

# displayed as horizontal bar plot
plt.figure(figsize=(8,6))
plt.barh(df_top15_math["feature"][::-1], df_top15_math["importance"][::-1])
plt.title("Top 15 Predictors – Maths (Permutation Importance)")
plt.xlabel("Importance")
plt.tight_layout()
plt.show()

"""## OLS Model - Anh Tran"""

import pandas as pd
import numpy as np
from sklearn.linear_model import LinearRegression
from sklearn.preprocessing import OneHotEncoder
from sklearn.compose import ColumnTransformer

db_sample = base_df.copy()

def clean_data(df: pd.DataFrame) -> pd.DataFrame:
    df["birth_year"] = df["birth"].str.split(' ').str[0]
    df["birth_qtr"] = df["birth"].str.split(' ').str[1]
    df = df.drop(columns=['birth'])
    df = df.dropna()
    return df

db_sample = clean_data(db_sample)

def one_hot_encode(df: pd.DataFrame) -> pd.DataFrame:
    # schooldistrict_id REMOVED
    categorical_features = [
        "gender",
        "ethnicity",
        "birth_year",
        "birth_qtr",
        "lunch",
        "class_type",
        "school",
        "degree",
        "ladder",
        "t_ethnicity",
        "school_id"      # keep school_id only
    ]

    numerical_features = ["experience"]

    preprocessor = ColumnTransformer(
        transformers=[
            ("cat", OneHotEncoder(), categorical_features),
            ("num", "passthrough", numerical_features)
        ])

    transformed_data = preprocessor.fit_transform(df)
    return transformed_data

X = one_hot_encode(db_sample)
y_math = db_sample["score_math"]
y_read = db_sample["score_read"]

# create train/val and test subsets on 80/20 split
X_trainval, X_test, y_read_trainval, y_read_test, y_math_trainval, y_math_test = train_test_split(
    X, y_read, y_math, test_size=0.2, random_state=42
)

# Evaluate the OLS model using repeated K-Fold CV.
# Returns mean MSE, std of MSE, and list of all MSEs

def eval_ols(X, y, n_splits=5, n_repeats=10, random_state=99):
    rkf = RKF(n_splits=n_splits, n_repeats=n_repeats, random_state=random_state)
    mses = []

    for train_idx, val_idx in rkf.split(X):
        X_train, X_val = X[train_idx], X[val_idx]
        y_train, y_val = y.iloc[train_idx], y.iloc[val_idx]

        model = LinearRegression()
        model.fit(X_train, y_train)
        preds = model.predict(X_val)
        mse = MSE(y_val, preds)
        mses.append(mse)

    return np.mean(mses), np.std(mses), mses

from sklearn.metrics import mean_squared_error

# evaluate with repeated K-Fold (10x5)
#reading
OLS_mean_mse_read_rkf, OLS_std_mse_read_rkf, OLS_mses_read_rkf = eval_ols(
    X_trainval,
    y_read_trainval,
    n_splits=5,
    n_repeats=10,
    random_state=99
)

print("OLS (Reading) – Repeated 10x5 K-Fold on trainval")
print(f"Mean MSE (trainval RKF): {OLS_mean_mse_read_rkf:.3f}")
print(f"Std MSE  (trainval RKF): {OLS_std_mse_read_rkf:.3f}")

#maths
OLS_mean_mse_math_rkf, OLS_std_mse_math_rkf, OLS_mses_math_rkf = eval_ols(
    X_trainval,
    y_math_trainval,
    n_splits=5,
    n_repeats=10,
    random_state=99
)

print("\nOLS (Maths) – Repeated 10x5 K-Fold on trainval")
print(f"Mean MSE (trainval RKF): {OLS_mean_mse_math_rkf:.3f}")
print(f"Std MSE  (trainval RKF): {OLS_std_mse_math_rkf:.3f}")

# Train the model for reading
model_read = LinearRegression().fit(X_trainval, y_read_trainval)

# Get predictions on the test set for reading
y_pred_read = model_read.predict(X_test)

# Calculate R-squared (score)
r2_read = model_read.score(X_test, y_read_test)
print("\n\nFinal evaluation (Reading):")
print(f"Reading Model R-squared: {r2_read:.3f}")

# Calculate MSE
mse_read = mean_squared_error(y_read_test, y_pred_read)
print(f"Reading Model MSE: {mse_read:.3f}\n")


# Train the model for maths
model_math = LinearRegression().fit(X_trainval, y_math_trainval)

# Get predictions on the test set for maths
y_pred_math = model_math.predict(X_test)

# Calculate R-squared (score)
r2_math = model_math.score(X_test, y_math_test)
print("Final evaluation (Maths):")
print(f"Math Model R-squared: {r2_math:.3f}")

# Calculate MSE
mse_math = mean_squared_error(y_math_test, y_pred_math)
print(f"Math Model MSE: {mse_math:.3f}")


print(f"\nCombined MSE on test set: {(0.5 * (mse_read + mse_math)):.3f}")

# GET BIGGEST DRIVERS FOR READING AND MATH

# Rebuild the preprocessor so we can get feature names
categorical_features = [
    "gender", "ethnicity", "birth_year", "birth_qtr",
    "lunch", "class_type", "school", "degree",
    "ladder", "t_ethnicity", "school_id"   # schooldistrict_id removed
]
numerical_features = ["experience"]

preprocessor = ColumnTransformer(
    transformers=[
        ("cat", OneHotEncoder(handle_unknown="ignore"), categorical_features),
        ("num", "passthrough", numerical_features)
    ]
)

# Fit on training data to get feature names
preprocessor.fit(db_sample)

# Get one-hot encoded feature names
ohe = preprocessor.named_transformers_["cat"]
cat_feature_names = ohe.get_feature_names_out(categorical_features)

# Combine with numerical features
feature_names = list(cat_feature_names) + numerical_features

# ---- Reading score drivers ----
coef_read = model_read.coef_
drivers_read = pd.DataFrame({
    "feature": feature_names,
    "coef": coef_read,
    "abs_coef": np.abs(coef_read)
}).sort_values("abs_coef", ascending=False)

print("\n===== TOP DRIVERS FOR READING SCORE =====")
print(drivers_read.head(15))

# ---- Math score drivers ----
coef_math = model_math.coef_
drivers_math = pd.DataFrame({
    "feature": feature_names,
    "coef": coef_math,
    "abs_coef": np.abs(coef_math)
}).sort_values("abs_coef", ascending=False)

print("\n===== TOP DRIVERS FOR MATH SCORE =====")
print(drivers_math.head(15))

import matplotlib.pyplot as plt
import numpy as np

# Category + Color Mapping
category_colors = {
    "gender": "tab:blue",
    "ethnicity": "tab:orange",
    "birth_year": "tab:green",
    "birth_qtr": "tab:red",
    "lunch": "tab:purple",
    "class_type": "tab:brown",
    "school": "tab:pink",
    "degree": "tab:gray",
    "ladder": "tab:olive",
    "t_ethnicity": "tab:cyan",
    "experience": "tab:blue",
    # any other category = black
}

def get_feature_category(feature_name):
    """Match a feature back to its category prefix."""
    for cat in category_colors.keys():
        if feature_name.startswith(cat):
            return cat
    return "other"


def plot_top_drivers(df, title):
    top = df.head(15).copy()

    # Map category to each feature
    top["category"] = top["feature"].apply(get_feature_category)
    top["color"] = top["category"].map(category_colors).fillna("black")

    # Plot
    plt.figure(figsize=(12, 7))
    plt.barh(top["feature"], top["abs_coef"], color=top["color"])
    plt.gca().invert_yaxis()

    plt.xlabel("Absolute Coefficient")
    plt.title(title)

    # Build legend from categories actually present
    cats = top["category"].unique()
    handles = [plt.Rectangle((0,0),1,1, color=category_colors.get(cat, "black")) for cat in cats]
    plt.legend(handles, cats, title="Feature Category")

    plt.tight_layout()
    plt.show()



# MAKE THE PLOTS


plot_top_drivers(drivers_read, "Top 15 Drivers of Reading Score (Color-Grouped)")
plot_top_drivers(drivers_math, "Top 15 Drivers of Math Score (Color-Grouped)")

rmse_read = np.sqrt(mse_read)
rmse_math = np.sqrt(mse_math)

print(f"Reading RMSE: {rmse_read:.4f}")
print(f"Math RMSE: {rmse_math:.4f}")

# Get necessary variables
R2_read = model_read.score(X_test, y_read_test)
n = db_sample.shape[0]        # Number of samples
p = X.shape[1]                # Number of predictors (features)

# Calculate Adjusted R^2 for Reading
adj_R2_read = 1 - (1 - R2_read) * ((n - 1) / (n - p - 1))

# Calculate Adjusted R^2 for Math
R2_math = model_math.score(X_test, y_math_test)
adj_R2_math = 1 - (1 - R2_math) * ((n - 1) / (n - p - 1))

print(f"Reading Adjusted R^2: {adj_R2_read:.4f}")
print(f"Math Adjusted R^2: {adj_R2_math:.4f}")

"""## Gradient Boosting - Neha Gajendra

### Predicting score_math

#### Load Data
"""

# INSTALL + IMPORTS
!pip install tabulate
from tabulate import tabulate
import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt
import numpy as np
from sklearn.ensemble import GradientBoostingRegressor
from sklearn.metrics import mean_squared_error
from sklearn.model_selection import train_test_split, cross_val_score, KFold

# LOAD DATA
db = base_df.copy()
# VISUAL SETTINGS
pd.set_option('display.colheader_justify', 'center')
pd.set_option('display.width', 300)
pd.set_option('display.max_columns', 20)

def pretty(df, n=5):
    print(tabulate(df.head(n), headers='keys', tablefmt='grid', showindex=True))

# COPY BASE DATA
X = db.copy()

# BASIC CLEANING
X = X.drop(columns=['score_read'], errors='ignore')
X = X.dropna(subset=['score_math'])
X = X.drop(columns=['schooldistrict_id'], errors='ignore')

# TARGET ENCODING FOR SCHOOL_ID
from sklearn.model_selection import KFold

X_temp = X.copy()
kf = KFold(n_splits=5, shuffle=True, random_state=1)

# New column for the encoded means
X_temp['school_mean_score'] = np.nan

for train_idx, val_idx in kf.split(X_temp):
    train_data = X_temp.iloc[train_idx]
    val_data = X_temp.iloc[val_idx]

    # Compute mean score per school on the training data
    school_means = train_data.groupby('school_id')['score_math'].mean()

    # Map the training means onto the validation fold
    mapped_means = val_data['school_id'].map(school_means)

    # Safely assign using iloc (positional)
    X_temp.iloc[val_idx, X_temp.columns.get_loc('school_mean_score')] = mapped_means

# Fill unseen schools with global mean
global_mean = X_temp['score_math'].mean()
X_temp['school_mean_score'].fillna(global_mean, inplace=True)

# Drop the original school_id (we’ve encoded it)
X_temp.drop(columns=['school_id'], inplace=True, errors='ignore')

# Replace back into main dataset
X = X_temp

# ENCODING + IMPUTATION
# Ordinal encode ladder
ladder_order = {'probation': 0, 'apprentice': 1, 'level1': 2, 'level2': 3, 'level3': 4}
X['ladder'] = X['ladder'].map(ladder_order)

# Explicitly fill NaNs in ladder immediately after mapping, as it's now numeric
if X['ladder'].isnull().any():
    X['ladder'] = X['ladder'].fillna(X['ladder'].median())

# One-hot encode categorical variables
one_hot_cols = [
    'gender', 'class_type', 'lunch', 'ethnicity',
    'school', 'degree', 't_ethnicity',
    'birth'
]
X = pd.get_dummies(X, columns=one_hot_cols, drop_first=True)

# HANDLE MISSING VALUES
numeric_cols = X.select_dtypes(include=[np.number]).columns
X[numeric_cols] = X[numeric_cols].fillna(X[numeric_cols].median())

categorical_cols = X.select_dtypes(exclude=[np.number]).columns
for col in categorical_cols:
    X[col] = X[col].fillna(X[col].mode()[0])

# Verify that no NaNs remain after imputation
print("Total NaNs after imputation:", X.isnull().sum().sum())

# Handle Outliers for Target
q1 = X['score_math'].quantile(0.25)
q3 = X['score_math'].quantile(0.75)
iqr = q3 - q1
lower = q1 - 1.5 * iqr
upper = q3 + 1.5 * iqr

X['score_math'] = X['score_math'].clip(lower, upper)

q1 = db['experience'].quantile(0.25)
q3 = db['experience'].quantile(0.75)
iqr = q3 - q1
upper = q3 + 1.5 * iqr

# drop rows with extreme high experience
db = db[db['experience'] <= upper]


# SPLIT FEATURES + TARGET
y_math = X['score_math']
X = X.drop(columns=['score_math'])

# SPLIT TRAIN/TEST SET
X_train, X_test, y_train, y_test = train_test_split(
    X, y_math, test_size=0.2, random_state=1
)

"""### Model"""

# MODEL + CROSS VALIDATION
params = {
    'n_estimators': 500,
    'learning_rate': 0.0325,
    'max_depth': 3,
    'subsample': 0.7,
    'random_state': 1
}

model = GradientBoostingRegressor(**params)

# 5-fold CV (shuffle for randomness)
cv = KFold(n_splits=5, shuffle=True, random_state=42)


scores = cross_val_score(model, X, y_math, scoring='neg_mean_squared_error', cv=cv, n_jobs=-1)

mse_scores = -scores
print("\nMSE for each fold:", mse_scores)
print("Mean CV MSE:", round(np.mean(mse_scores), 2))
print("Std deviation:", round(np.std(mse_scores), 2))

# Final fit + baseline test performance
model.fit(X_train, y_train)
y_pred = model.predict(X_test)
print("\nTest MSE:", round(mean_squared_error(y_test, y_pred), 2))

for i, (train_idx, val_idx) in enumerate(cv.split(X)):
    if i == 3:  # 4th fold
        bad_fold = X.iloc[val_idx]
        print(bad_fold.describe())

from sklearn.model_selection import StratifiedKFold
y_bins = pd.qcut(y_math, q=5, labels=False)
cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=1)
scores = cross_val_score(model, X, y_math, scoring='neg_mean_squared_error', cv=cv.split(X, y_bins))

mse_scores = -scores
print("\nMSE for each fold:", mse_scores)
print("Mean CV MSE:", round(np.mean(mse_scores), 2))
print("Std deviation:", round(np.std(mse_scores), 2))

# Final fit + baseline test performance
model.fit(X_train, y_train)
y_pred = model.predict(X_test)
print("\nTest MSE:", round(mean_squared_error(y_test, y_pred), 2))

from sklearn.metrics import mean_squared_error

# baseline
params = {
    'n_estimators': 500,
    'learning_rate': 0.0325,
    'max_depth': 4, # This was the depth used for the printed baseline MSE
    'subsample': 0.7,
    'random_state': 1
}

best_model = GradientBoostingRegressor(**params).fit(X_train, y_train)
base_mse = mean_squared_error(y_test, best_model.predict(X_test))
print("Baseline MSE:", base_mse)

print("\nTrying more trees (n_estimators):")
for n in [700, 1000, 1500]:
    m = GradientBoostingRegressor(**{**params, 'n_estimators': n}).fit(X_train, y_train)
    mse = mean_squared_error(y_test, m.predict(X_test))
    print(f"n_estimators={n} -> MSE={mse:.4f}")

print("\nTrying smaller learning rates with more trees (learning_rate):")
# Use a larger n_estimators (e.g., 1000) when reducing learning rate
for lr in [0.03, 0.025, 0.02, 0.03255]:
    m = GradientBoostingRegressor(**{**params, 'learning_rate': lr, 'n_estimators': 1000}).fit(X_train, y_train)
    mse = mean_squared_error(y_test, m.predict(X_test))
    print(f"learning_rate={lr} -> MSE={mse:.4f}")

print("\nTrying slightly deeper trees (max_depth):")
for depth in [3, 5]:
    m = GradientBoostingRegressor(**{**params, 'max_depth': depth}).fit(X_train, y_train)
    mse = mean_squared_error(y_test, m.predict(X_test))
    print(f"max_depth={depth} -> MSE={mse:.4f}")

# The subsampling tweak was already completed in the subsequent cell and found 0.7 to be optimal
# so this part of tuning is now completed.

# Try subsampling tweak
for sub in [0.6, 0.5, 0.9, 0.8, 0.7]:
    m = GradientBoostingRegressor(**{**params, 'subsample': sub}).fit(X_train, y_train)
    mse = mean_squared_error(y_test, m.predict(X_test))
    print(f"subsample={sub} -> MSE={mse:.4f}")

rmse = np.sqrt(mean_squared_error(y_test, y_pred))
print("RMSE:", round(rmse, 2))

pretty(X.head())

importances = pd.Series(model.feature_importances_, index=X.columns)
print(importances.sort_values(ascending=False).head(15))

from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score
import numpy as np

y_pred_train = model.predict(X_train)
y_pred_test  = model.predict(X_test)

mse_train = mean_squared_error(y_train, y_pred_train)
mse_test  = mean_squared_error(y_test, y_pred_test)
rmse_test = np.sqrt(mse_test)
mae_test  = mean_absolute_error(y_test, y_pred_test)
r2_test   = r2_score(y_test, y_pred_test)

print(f"Train MSE: {mse_train:.2f}")
print(f"Test MSE: {mse_test:.2f}")
print(f"Test RMSE: {rmse_test:.2f}")
print(f"Test MAE: {mae_test:.2f}")
print(f"Test R\u00B2: {r2_test:.3f}")

"""### Prediciting score_read

#### Load Data
"""

# LOAD DATA
db = base_df.copy()

# VISUAL SETTINGS
pd.set_option('display.colheader_justify', 'center')
pd.set_option('display.width', 300)
pd.set_option('display.max_columns', 20)

def pretty(df, n=5):
    print(tabulate(df.head(n), headers='keys', tablefmt='grid', showindex=True))

# COPY BASE DATA
X = db.copy()

# BASIC CLEANING
X = X.drop(columns=['score_math'], errors='ignore')
X = X.dropna(subset=['score_read'])
X = X.drop(columns=['schooldistrict_id'], errors='ignore')

# TARGET ENCODING FOR SCHOOL_ID
from sklearn.model_selection import KFold

X_temp = X.copy()
kf = KFold(n_splits=5, shuffle=True, random_state=1)

# New column for the encoded means
X_temp['school_mean_score'] = np.nan

for train_idx, val_idx in kf.split(X_temp):
    train_data = X_temp.iloc[train_idx]
    val_data = X_temp.iloc[val_idx]

    # Compute mean score per school on the training data
    school_means = train_data.groupby('school_id')['score_read'].mean()

    # Map the training means onto the validation fold
    mapped_means = val_data['school_id'].map(school_means)

    # Safely assign using iloc (positional)
    X_temp.iloc[val_idx, X_temp.columns.get_loc('school_mean_score')] = mapped_means

# Fill unseen schools with global mean
global_mean = X_temp['score_read'].mean()
X_temp['school_mean_score'].fillna(global_mean, inplace=True)

# Drop the original school_id (we’ve encoded it)
X_temp.drop(columns=['school_id'], inplace=True, errors='ignore')

# Replace back into main dataset
X = X_temp

# ENCODING + IMPUTATION
# Ordinal encode ladder
ladder_order = {'probation': 0, 'apprentice': 1, 'level1': 2, 'level2': 3, 'level3': 4}
X['ladder'] = X['ladder'].map(ladder_order)

# Explicitly fill NaNs in ladder immediately after mapping, as it's now numeric
if X['ladder'].isnull().any():
    X['ladder'] = X['ladder'].fillna(X['ladder'].median())

# One-hot encode categorical variables
one_hot_cols = [
    'gender', 'class_type', 'lunch', 'ethnicity',
    'school', 'degree', 't_ethnicity',
    'birth'
]
X = pd.get_dummies(X, columns=one_hot_cols, drop_first=True)

# HANDLE MISSING VALUES
numeric_cols = X.select_dtypes(include=[np.number]).columns
X[numeric_cols] = X[numeric_cols].fillna(X[numeric_cols].median())

categorical_cols = X.select_dtypes(exclude=[np.number]).columns
for col in categorical_cols:
    X[col] = X[col].fillna(X[col].mode()[0])

# Verify that no NaNs remain after imputation
print("Total NaNs after imputation:", X.isnull().sum().sum())

# Handle Outliers for Target
q1 = X['score_read'].quantile(0.25)
q3 = X['score_read'].quantile(0.75)
iqr = q3 - q1
lower = q1 - 1.5 * iqr
upper = q3 + 1.5 * iqr

X['score_read'] = X['score_read'].clip(lower, upper)

q1 = db['experience'].quantile(0.25)
q3 = db['experience'].quantile(0.75)
iqr = q3 - q1
upper = q3 + 1.5 * iqr

# drop rows with extreme high experience
db = db[db['experience'] <= upper]


# SPLIT FEATURES + TARGET
y_read = X['score_read']
X = X.drop(columns=['score_read'])

# SPLIT TRAIN/TEST SET
X_train, X_test, y_train, y_test = train_test_split(
    X, y_read, test_size=0.2, random_state=1
)

"""#### Model"""

# MODEL + CROSS VALIDATION
params = {
    'n_estimators': 500,
    'learning_rate': 0.0325,
    'max_depth': 3,
    'subsample': 0.7,
    'random_state': 1
}

model = GradientBoostingRegressor(**params)

# 5-fold CV (shuffle for randomness)
cv = KFold(n_splits=5, shuffle=True, random_state=42)


scores = cross_val_score(model, X, y_read, scoring='neg_mean_squared_error', cv=cv, n_jobs=-1)

mse_scores = -scores
print("\nMSE for each fold:", mse_scores)
print("Mean CV MSE:", round(np.mean(mse_scores), 2))
print("Std deviation:", round(np.std(mse_scores), 2))

# Final fit + baseline test performance
model.fit(X_train, y_train)
y_pred = model.predict(X_test)
print("\nTest MSE:", round(mean_squared_error(y_test, y_pred), 2))

for i, (train_idx, val_idx) in enumerate(cv.split(X)):
    if i == 3:  # 4th fold
        bad_fold = X.iloc[val_idx]
        print(bad_fold.describe())

from sklearn.model_selection import StratifiedKFold
y_bins = pd.qcut(y_read, q=5, labels=False)
cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=1)
scores = cross_val_score(model, X, y_read, scoring='neg_mean_squared_error', cv=cv.split(X, y_bins))

mse_scores = -scores
print("\nMSE for each fold:", mse_scores)
print("Mean CV MSE:", round(np.mean(mse_scores), 2))
print("Std deviation:", round(np.std(mse_scores), 2))

# Final fit + baseline test performance
model.fit(X_train, y_train)
y_pred = model.predict(X_test)
print("\nTest MSE:", round(mean_squared_error(y_test, y_pred), 2))

from sklearn.metrics import mean_squared_error

# baseline
params = {
    'n_estimators': 500,
    'learning_rate': 0.0325,
    'max_depth': 4, # This was the depth used for the printed baseline MSE
    'subsample': 0.7,
    'random_state': 1
}

best_model = GradientBoostingRegressor(**params).fit(X_train, y_train)
base_mse = mean_squared_error(y_test, best_model.predict(X_test))
print("Baseline MSE:", base_mse)

print("\nTrying more trees (n_estimators):")
for n in [700, 1000, 1500]:
    m = GradientBoostingRegressor(**{**params, 'n_estimators': n}).fit(X_train, y_train)
    mse = mean_squared_error(y_test, m.predict(X_test))
    print(f"n_estimators={n} -> MSE={mse:.4f}")

print("\nTrying smaller learning rates with more trees (learning_rate):")
# Use a larger n_estimators (e.g., 1000) when reducing learning rate
for lr in [0.03, 0.025, 0.02, 0.03255]:
    m = GradientBoostingRegressor(**{**params, 'learning_rate': lr, 'n_estimators': 1000}).fit(X_train, y_train)
    mse = mean_squared_error(y_test, m.predict(X_test))
    print(f"learning_rate={lr} -> MSE={mse:.4f}")

print("\nTrying slightly deeper trees (max_depth):")
for depth in [3, 5]:
    m = GradientBoostingRegressor(**{**params, 'max_depth': depth}).fit(X_train, y_train)
    mse = mean_squared_error(y_test, m.predict(X_test))
    print(f"max_depth={depth} -> MSE={mse:.4f}")

# The subsampling tweak was already completed in the subsequent cell and found 0.7 to be optimal
# so this part of tuning is now completed.

# Try subsampling tweak
for sub in [0.6, 0.5, 0.9, 0.8, 0.7]:
    m = GradientBoostingRegressor(**{**params, 'subsample': sub}).fit(X_train, y_train)
    mse = mean_squared_error(y_test, m.predict(X_test))
    print(f"subsample={sub} -> MSE={mse:.4f}")

rmse = np.sqrt(mean_squared_error(y_test, y_pred))
print("RMSE:", round(rmse, 2))

pretty(X.head())

importances = pd.Series(model.feature_importances_, index=X.columns)
print(importances.sort_values(ascending=False).head(15))

from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score
import numpy as np

y_pred_train = model.predict(X_train)
y_pred_test  = model.predict(X_test)

mse_train = mean_squared_error(y_train, y_pred_train)
mse_test  = mean_squared_error(y_test, y_pred_test)
rmse_test = np.sqrt(mse_test)
mae_test  = mean_absolute_error(y_test, y_pred_test)
r2_test   = r2_score(y_test, y_pred_test)

print(f"Train MSE: {mse_train:.2f}")
print(f"Test MSE: {mse_test:.2f}")
print(f"Test RMSE: {rmse_test:.2f}")
print(f"Test MAE: {mae_test:.2f}")
print(f"Test R\u00B2: {r2_test:.3f}")

"""## XGBoost - Neha Gajendra

### Predicting score_math

#### Load Data
"""

# INSTALL + IMPORTS
!pip install tabulate
!pip install xgboost
from xgboost import XGBRegressor
from tabulate import tabulate
import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt
import numpy as np
from sklearn.ensemble import GradientBoostingRegressor
from sklearn.metrics import mean_squared_error
from sklearn.model_selection import train_test_split, cross_val_score, KFold

# LOAD DATA
db = base_df.copy()

# VISUAL SETTINGS
pd.set_option('display.colheader_justify', 'center')
pd.set_option('display.width', 300)
pd.set_option('display.max_columns', 20)

def pretty(df, n=5):
    print(tabulate(df.head(n), headers='keys', tablefmt='grid', showindex=True))

# COPY BASE DATA
X = db.copy()

# BASIC CLEANING
X = X.drop(columns=['score_read'], errors='ignore')
X = X.dropna(subset=['score_math'])
X = X.drop(columns=['schooldistrict_id'], errors='ignore')
# X = X.drop(columns=['school_id'], errors='ignore')

# TARGET ENCODING FOR SCHOOL_ID
from sklearn.model_selection import KFold

X_temp = X.copy()
kf = KFold(n_splits=5, shuffle=True, random_state=1)

# New column for the encoded means
X_temp['school_mean_score'] = np.nan

for train_idx, val_idx in kf.split(X_temp):
    train_data = X_temp.iloc[train_idx]
    val_data = X_temp.iloc[val_idx]

    # Compute mean score per school on the training data
    school_means = train_data.groupby('school_id')['score_math'].mean()

    # Map the training means onto the validation fold
    mapped_means = val_data['school_id'].map(school_means)

    # Safely assign using iloc (positional)
    X_temp.iloc[val_idx, X_temp.columns.get_loc('school_mean_score')] = mapped_means

# Fill unseen schools with global mean
global_mean = X_temp['score_math'].mean()
X_temp['school_mean_score'].fillna(global_mean, inplace=True)

# Drop the original school_id (we’ve encoded it)
X_temp.drop(columns=['school_id'], inplace=True, errors='ignore')

# Replace back into main dataset
X = X_temp

# ENCODING + IMPUTATION
# Ordinal encode ladder
ladder_order = {'probation': 0, 'apprentice': 1, 'level1': 2, 'level2': 3, 'level3': 4}
X['ladder'] = X['ladder'].map(ladder_order)

# Explicitly fill NaNs in ladder immediately after mapping, as it's now numeric
if X['ladder'].isnull().any():
    X['ladder'] = X['ladder'].fillna(X['ladder'].median())

# One-hot encode categorical variables
one_hot_cols = [
    'gender', 'class_type', 'lunch', 'ethnicity',
    'school', 'degree', 't_ethnicity',
    'birth'
]
X = pd.get_dummies(X, columns=one_hot_cols, drop_first=True)

# HANDLE MISSING VALUES
numeric_cols = X.select_dtypes(include=[np.number]).columns
X[numeric_cols] = X[numeric_cols].fillna(X[numeric_cols].median())

categorical_cols = X.select_dtypes(exclude=[np.number]).columns
for col in categorical_cols:
    X[col] = X[col].fillna(X[col].mode()[0])

# Verify that no NaNs remain after imputation
print("Total NaNs after imputation:", X.isnull().sum().sum())

# Handle Outliers for Target
q1 = X['score_math'].quantile(0.25)
q3 = X['score_math'].quantile(0.75)
iqr = q3 - q1
lower = q1 - 1.5 * iqr
upper = q3 + 1.5 * iqr

X['score_math'] = X['score_math'].clip(lower, upper)

q1 = db['experience'].quantile(0.25)
q3 = db['experience'].quantile(0.75)
iqr = q3 - q1
upper = q3 + 1.5 * iqr

# drop rows with extreme high experience
db = db[db['experience'] <= upper]


# SPLIT FEATURES + TARGET
y_math = X['score_math']
X = X.drop(columns=['score_math'])

# PLIT TRAIN/TEST SET
X_train, X_test, y_train, y_test = train_test_split(
    X, y_math, test_size=0.2, random_state=1
)

"""#### Tuning"""

## this cell is for tuning
from sklearn.model_selection import GridSearchCV, KFold
from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score

# BASE MODEL (good defaults)
xgb_base = XGBRegressor(
    objective='reg:squarederror',
    tree_method='hist',
    random_state=1,
    n_estimators=700,
    learning_rate=0.03,
    max_depth=3,
    subsample=0.8,
    colsample_bytree=0.8,
    reg_alpha=0.1,
    reg_lambda=2
)

# STEP 1: Tune learning_rate + max_depth
param_grid_1 = {
    'learning_rate': [0.015, 0.02, 0.03],
    'max_depth': [2, 3, 4]
}

cv = KFold(n_splits=5, shuffle=True, random_state=1)

search1 = GridSearchCV(
    estimator=xgb_base,
    param_grid=param_grid_1,
    scoring='neg_mean_squared_error',
    cv=cv,
    n_jobs=-1,
    verbose=1
)

search1.fit(X_train, y_train)
print("Step 1 Best Params:", search1.best_params_)

# Update model
best_params = search1.best_params_
xgb_stage2 = search1.best_estimator_

# STEP 2: Tune sampling
param_grid_2 = {
    'subsample': [0.7, 0.8, 0.9],
    'colsample_bytree': [0.7, 0.8, 0.9]
}

search2 = GridSearchCV(
    estimator=xgb_stage2,
    param_grid=param_grid_2,
    scoring='neg_mean_squared_error',
    cv=cv,
    n_jobs=-1,
    verbose=1
)

search2.fit(X_train, y_train)
print("Step 2 Best Params:", search2.best_params_)

best_params.update(search2.best_params_)
xgb_stage3 = search2.best_estimator_

# STEP 3: Tune regularisation
param_grid_3 = {
    'reg_alpha': [0, 0.1, 0.3, 1],
    'reg_lambda': [1, 2, 3]
}

search3 = GridSearchCV(
    estimator=xgb_stage3,
    param_grid=param_grid_3,
    scoring='neg_mean_squared_error',
    cv=cv,
    n_jobs=-1,
    verbose=1
)

search3.fit(X_train, y_train)
print("Step 3 Best Params:", search3.best_params_)

best_params.update(search3.best_params_)
final_xgb = search3.best_estimator_

# FINAL REPORTING
y_pred = final_xgb.predict(X_test)

mse  = mean_squared_error(y_test, y_pred)
rmse = np.sqrt(mse)
mae  = mean_absolute_error(y_test, y_pred)
r2   = r2_score(y_test, y_pred)

print("\nFINAL XGBOOST RESULTS")
print("----------------------------")
print("MSE :", mse)
print("RMSE:", rmse)
print("MAE :", mae)
print("R\u00B2  :", r2)
print("\nFINAL BEST PARAMETERS:", best_params)

!pip install xgboost
from xgboost import XGBRegressor
from sklearn.model_selection import RandomizedSearchCV, KFold
import numpy as np

param_dist = {
    # Include your good values AND a broad search
    'n_estimators': [300, 500, 700, 900, 1200, 1500, 2000],

    # Very important: small learning rates
    'learning_rate': [0.005, 0.01, 0.015, 0.02, 0.03, 0.0325, 0.04],

    # Expand tree depth a bit
    'max_depth': [3, 4, 5, 6, 7],

    # Subsample and colsample — keep moderate ranges
    'subsample': np.linspace(0.6, 0.7, 0.9, 0.8),
    'colsample_bytree': np.linspace(0.6, 1.0, 9),

    # Regularisation terms
    'reg_lambda': np.logspace(-2, 1, 10),
    'reg_alpha': np.logspace(-2, 1, 10),

    # Child weight & gamma
    'min_child_weight': [1, 3, 5, 7],
    'gamma': np.linspace(0, 3, 7),
}

xgb = XGBRegressor(
    objective="reg:squarederror",
    tree_method="hist",
    random_state=1
)

cv = KFold(n_splits=5, shuffle=True, random_state=1)

search = RandomizedSearchCV(
    estimator=xgb,
    param_distributions=param_dist,
    n_iter=500,
    scoring="neg_mean_squared_error",
    n_jobs=-1,
    cv=cv,
    verbose=2,
    random_state=1
)

search.fit(X, y_math)

print("BEST PARAMS =", search.best_params_)
print("BEST CV MSE =", -search.best_score_)

"""#### Model"""

# MODEL + CROSS VALIDATION (XGBoost VERSION)
xgb_params = {
    'objective': 'reg:squarederror',
'tree_method': 'hist',

'n_estimators': 500,
'learning_rate': 0.02,
'max_depth': 5,

'subsample': 0.7875,
'colsample_bytree': 0.8,

'reg_alpha': 0.1,
'reg_lambda': 2,

'random_state': 1
}
model = XGBRegressor(**xgb_params)

# 5-fold CV (shuffle for randomness)
cv = KFold(n_splits=5, shuffle=True, random_state=42)

scores = cross_val_score(model, X, y_math, scoring='neg_mean_squared_error', cv=cv, n_jobs=-1)
mse_scores = -scores

print("\nGBoost MSE for each fold:", mse_scores)
print("Mean CV MSE:", round(np.mean(mse_scores), 2))
print("Std deviation:", round(np.std(mse_scores), 2))

# Fit final model
model.fit(X_train, y_train)
y_pred = model.predict(X_test)

# print("\nXGBoost Test MSE:", round(mean_squared_error(y_test, y_pred), 2))

mse  = mean_squared_error(y_test, y_pred)
rmse = np.sqrt(mse)
mae  = mean_absolute_error(y_test, y_pred)
r2   = r2_score(y_test, y_pred)

print("\nFINAL XGBOOST RESULTS")
print("----------------------------")
print("MSE :", mse)
print("RMSE:", rmse)
print("MAE :", mae)
print("R²  :", r2)

from sklearn.metrics import mean_squared_error

print("\nHyperparameter Sweep (XGBoost)")

best_mse = float('inf')
best_params = None

for lr in [0.02, 0.03]:
    for depth in [3,4]:
        for subs in [0.7,0.8]:

            model = XGBRegressor(
                objective='reg:squarederror',
                n_estimators=500,
                learning_rate=lr,
                max_depth=depth,
                subsample=subs,
                colsample_bytree=0.8,
                reg_alpha=0.1,
                reg_lambda=2.0,
                random_state=1
            )

            scores = -cross_val_score(model, X, y_math,
                                      scoring='neg_mean_squared_error',
                                      cv=5, n_jobs=-1)

            mean_mse = scores.mean()

            print(f"LR={lr}, depth={depth}, subs={subs} → MSE={mean_mse:.2f}")

            if mean_mse < best_mse:
                best_mse = mean_mse
                best_params = (lr, depth, subs)

print("\nBest XGB params:", best_params, "| MSE:", round(best_mse,2))

importances = pd.Series(model.feature_importances_, index=X.columns)
print(importances.sort_values(ascending=False).head(15))

"""### Predicting score_read

#### Load Data
"""

# INSTALL + IMPORTS
!pip install tabulate
!pip install xgboost
from xgboost import XGBRegressor
from tabulate import tabulate
import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt
import numpy as np
from sklearn.ensemble import GradientBoostingRegressor
from sklearn.metrics import mean_squared_error
from sklearn.model_selection import train_test_split, cross_val_score, KFold

# LOAD DATA
db = base_df.copy()

# VISUAL SETTINGS
pd.set_option('display.colheader_justify', 'center')
pd.set_option('display.width', 300)
pd.set_option('display.max_columns', 20)

def pretty(df, n=5):
    print(tabulate(df.head(n), headers='keys', tablefmt='grid', showindex=True))

# COPY BASE DATA
X = db.copy()

# BASIC CLEANING
X = X.drop(columns=['score_math'], errors='ignore')
X = X.dropna(subset=['score_read'])
X = X.drop(columns=['schooldistrict_id'], errors='ignore')
# X = X.drop(columns=['school_id'], errors='ignore')

# TARGET ENCODING FOR SCHOOL_ID
from sklearn.model_selection import KFold

X_temp = X.copy()
kf = KFold(n_splits=5, shuffle=True, random_state=1)

# New column for the encoded means
X_temp['school_mean_score'] = np.nan

for train_idx, val_idx in kf.split(X_temp):
    train_data = X_temp.iloc[train_idx]
    val_data = X_temp.iloc[val_idx]

    # Compute mean score per school on the training data
    school_means = train_data.groupby('school_id')['score_read'].mean()

    # Map the training means onto the validation fold
    mapped_means = val_data['school_id'].map(school_means)

    # Safely assign using iloc (positional)
    X_temp.iloc[val_idx, X_temp.columns.get_loc('school_mean_score')] = mapped_means

# Fill unseen schools with global mean
global_mean = X_temp['score_read'].mean()
X_temp['school_mean_score'].fillna(global_mean, inplace=True)

# Drop the original school_id (we’ve encoded it)
X_temp.drop(columns=['school_id'], inplace=True, errors='ignore')

# Replace back into main dataset
X = X_temp

# ENCODING + IMPUTATION
# Ordinal encode ladder
ladder_order = {'probation': 0, 'apprentice': 1, 'level1': 2, 'level2': 3, 'level3': 4}
X['ladder'] = X['ladder'].map(ladder_order)

# Explicitly fill NaNs in ladder immediately after mapping, as it's now numeric
if X['ladder'].isnull().any():
    X['ladder'] = X['ladder'].fillna(X['ladder'].median())

# One-hot encode categorical variables
one_hot_cols = [
    'gender', 'class_type', 'lunch', 'ethnicity',
    'school', 'degree', 't_ethnicity',
    'birth'
]
X = pd.get_dummies(X, columns=one_hot_cols, drop_first=True)

# HANDLE MISSING VALUES
numeric_cols = X.select_dtypes(include=[np.number]).columns
X[numeric_cols] = X[numeric_cols].fillna(X[numeric_cols].median())

categorical_cols = X.select_dtypes(exclude=[np.number]).columns
for col in categorical_cols:
    X[col] = X[col].fillna(X[col].mode()[0])

# Verify that no NaNs remain after imputation
print("Total NaNs after imputation:", X.isnull().sum().sum())

# Handle Outliers for Target
q1 = X['score_read'].quantile(0.25)
q3 = X['score_read'].quantile(0.75)
iqr = q3 - q1
lower = q1 - 1.5 * iqr
upper = q3 + 1.5 * iqr

X['score_read'] = X['score_read'].clip(lower, upper)

q1 = db['experience'].quantile(0.25)
q3 = db['experience'].quantile(0.75)
iqr = q3 - q1
upper = q3 + 1.5 * iqr

# drop rows with extreme high experience
db = db[db['experience'] <= upper]


# SPLIT FEATURES + TARGET
y_read = X['score_read']
X = X.drop(columns=['score_read'])

# SPLIT TRAIN/TEST SET
X_train, X_test, y_train, y_test = train_test_split(
    X, y_read, test_size=0.2, random_state=1
)

"""#### Tuning"""

from xgboost import XGBRegressor
from sklearn.model_selection import GridSearchCV, KFold
from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score

# BASE MODEL (good defaults)
xgb_base = XGBRegressor(
    objective='reg:squarederror',
    tree_method='hist',
    random_state=1,
    n_estimators=700,
    learning_rate=0.03,
    max_depth=3,
    subsample=0.8,
    colsample_bytree=0.8,
    reg_alpha=0.1,
    reg_lambda=2
)

# STEP 1: Tune learning_rate + max_depth
param_grid_1 = {
    'learning_rate': [0.015, 0.02, 0.03],
    'max_depth': [2, 3, 4]
}

cv = KFold(n_splits=5, shuffle=True, random_state=1)

search1 = GridSearchCV(
    estimator=xgb_base,
    param_grid=param_grid_1,
    scoring='neg_mean_squared_error',
    cv=cv,
    n_jobs=-1,
    verbose=1
)

search1.fit(X_train, y_train)
print("Step 1 Best Params:", search1.best_params_)

# Update model
best_params = search1.best_params_
xgb_stage2 = search1.best_estimator_

# STEP 2: Tune sampling
param_grid_2 = {
    'subsample': [0.7, 0.8, 0.9],
    'colsample_bytree': [0.7, 0.8, 0.9]
}

search2 = GridSearchCV(
    estimator=xgb_stage2,
    param_grid=param_grid_2,
    scoring='neg_mean_squared_error',
    cv=cv,
    n_jobs=-1,
    verbose=1
)

search2.fit(X_train, y_train)
print("Step 2 Best Params:", search2.best_params_)

best_params.update(search2.best_params_)
xgb_stage3 = search2.best_estimator_

# STEP 3: Tune regularisation
param_grid_3 = {
    'reg_alpha': [0, 0.1, 0.3, 1],
    'reg_lambda': [1, 2, 3]
}

search3 = GridSearchCV(
    estimator=xgb_stage3,
    param_grid=param_grid_3,
    scoring='neg_mean_squared_error',
    cv=cv,
    n_jobs=-1,
    verbose=1
)

search3.fit(X_train, y_train)
print("Step 3 Best Params:", search3.best_params_)

best_params.update(search3.best_params_)
final_xgb = search3.best_estimator_

# FINAL REPORTING
y_pred = final_xgb.predict(X_test)

mse  = mean_squared_error(y_test, y_pred)
rmse = np.sqrt(mse)
mae  = mean_absolute_error(y_test, y_pred)
r2   = r2_score(y_test, y_pred)

print("\nFINAL XGBOOST RESULTS")
print("----------------------------")
print("MSE :", mse)
print("RMSE:", rmse)
print("MAE :", mae)
print("R\u00B2  :", r2)
print("\nFINAL BEST PARAMETERS:", best_params)

!pip install xgboost
from xgboost import XGBRegressor
from sklearn.model_selection import RandomizedSearchCV, KFold
import numpy as np

param_dist = {
    # Include your good values AND a broad search
    'n_estimators': [300, 500, 700, 900, 1200, 1500, 2000],

    # Very important: small learning rates
    'learning_rate': [0.005, 0.01, 0.015, 0.02, 0.03, 0.0325, 0.04],

    # Expand tree depth a bit
    'max_depth': [3, 4, 5, 6, 7],

    # Subsample and colsample — keep moderate ranges
    'subsample': np.linspace(0.6, 0.9, 10),
    'colsample_bytree': np.linspace(0.6, 1.0, 9),

    # Regularisation terms
    'reg_lambda': np.logspace(-2, 1, 10),
    'reg_alpha': np.logspace(-2, 1, 10),

    # Child weight & gamma
    'min_child_weight': [1, 3, 5, 7],
    'gamma': np.linspace(0, 3, 7),
}

xgb = XGBRegressor(
    objective="reg:squarederror",
    tree_method="hist",
    random_state=1
)

cv = KFold(n_splits=5, shuffle=True, random_state=1)

search = RandomizedSearchCV(
    estimator=xgb,
    param_distributions=param_dist,
    n_iter=200,
    scoring="neg_mean_squared_error",
    n_jobs=-1,
    cv=cv,
    verbose=2,
    random_state=1
)

search.fit(X, y_read)

print("BEST PARAMS =", search.best_params_)
print("BEST CV MSE =", -search.best_score_)

"""#### Model"""

# MODEL + CROSS VALIDATION (XGBoost VERSION)
xgb_params = {
    'objective': 'reg:squarederror',
'tree_method': 'hist',

'n_estimators': 500,
'learning_rate': 0.02,
'max_depth': 5,

'subsample': 0.7875,
'colsample_bytree': 0.8,

'reg_alpha': 0.1,
'reg_lambda': 2,

'random_state': 1
}
model = XGBRegressor(**xgb_params)

# 5-fold CV (shuffle for randomness)
cv = KFold(n_splits=5, shuffle=True, random_state=42)

scores = cross_val_score(model, X, y_read, scoring='neg_mean_squared_error', cv=cv, n_jobs=-1)
mse_scores = -scores

print("\nGBoost MSE for each fold:", mse_scores)
print("Mean CV MSE:", round(np.mean(mse_scores), 2))
print("Std deviation:", round(np.std(mse_scores), 2))

# Fit final model
model.fit(X_train, y_train)
y_pred = model.predict(X_test)

# print("\nXGBoost Test MSE:", round(mean_squared_error(y_test, y_pred), 2))

mse  = mean_squared_error(y_test, y_pred)
rmse = np.sqrt(mse)
mae  = mean_absolute_error(y_test, y_pred)
r2   = r2_score(y_test, y_pred)

print("\nFINAL XGBOOST RESULTS")
print("----------------------------")
print("MSE :", mse)
print("RMSE:", rmse)
print("MAE :", mae)
print("R\u00B2  :", r2)

from sklearn.metrics import mean_squared_error

print("\nHyperparameter Sweep (XGBoost)")

best_mse = float('inf')
best_params = None

for lr in [0.02, 0.03]:
    for depth in [3,4]:
        for subs in [0.7,0.8]:

            model = XGBRegressor(
                objective='reg:squarederror',
                n_estimators=500,
                learning_rate=lr,
                max_depth=depth,
                subsample=subs,
                colsample_bytree=0.8,
                reg_alpha=0.1,
                reg_lambda=2.0,
                random_state=1
            )

            scores = -cross_val_score(model, X, y_read,
                                      scoring='neg_mean_squared_error',
                                      cv=5, n_jobs=-1)

            mean_mse = scores.mean()

            print(f"LR={lr}, depth={depth}, subs={subs} → MSE={mean_mse:.2f}")

            if mean_mse < best_mse:
                best_mse = mean_mse
                best_params = (lr, depth, subs)

print("\nBest XGB params:", best_params, "| MSE:", round(best_mse,2))

importances = pd.Series(model.feature_importances_, index=X.columns)
print(importances.sort_values(ascending=False).head(15))

"""## Ridge & Lasso - Benan Nagshabandi"""

# metrics for score_math
import numpy as np
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.pipeline import make_pipeline
from sklearn.linear_model import RidgeCV, LassoCV
from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score

data = base_df.copy()

# selecting variables, excluded identifiers
cols = [
    "score_math", "gender", "lunch", "class_type",
    "experience", "ethnicity", "school", "degree",
    "ladder", "t_ethnicity"
]
data = data[cols]

# since some values are empty, replace with "missing"
for col in data.columns:
    if data[col].dtype == "object":
        data[col] = data[col].fillna("Missing")
    else:
        data[col] = data[col].fillna(data[col].median()) # if num use median

# design matrix
y = data["score_math"]
X = pd.get_dummies(data.drop(columns="score_math"), drop_first=True)

# training data with 80/20
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, random_state=123
)

# ridge and lasso model
alphas = np.logspace(-5, 5, 300)  # broader grid for finer tuning

ridge_model = make_pipeline(
    StandardScaler(),
    RidgeCV(alphas=alphas, cv=10)
).fit(X_train, y_train)

lasso_model = make_pipeline(
    StandardScaler(),
    LassoCV(alphas=alphas, cv=10, random_state=123, max_iter=10000)
).fit(X_train, y_train)

# Predictions
ridge_pred = ridge_model.predict(X_test)
lasso_pred = lasso_model.predict(X_test)

# function for metrics
def compute_metrics(y_true, y_pred, model_name, n_features):
    n = len(y_true)
    mse = mean_squared_error(y_true, y_pred)
    rmse = np.sqrt(mse)
    mae = mean_absolute_error(y_true, y_pred)
    mape = np.mean(np.abs((y_true - y_pred) / y_true)) * 100
    r2 = r2_score(y_true, y_pred)
    adj_r2 = 1 - ((1 - r2) * (n - 1)) / (n - n_features - 1)
    return {
        "Model": model_name,
        "MSE": mse,
        "RMSE": rmse,
        "MAE": mae,
        "MAPE": mape,
        "R2": r2,
        "Adjusted_R2": adj_r2
    }

ridge_p = np.sum(ridge_model.named_steps["ridgecv"].coef_ != 0)
lasso_p = np.sum(lasso_model.named_steps["lassocv"].coef_ != 0)

ridge_results = compute_metrics(y_test, ridge_pred, "Ridge", ridge_p)
lasso_results = compute_metrics(y_test, lasso_pred, "Lasso", lasso_p)

results_df = pd.DataFrame([ridge_results, lasso_results]).round(4)

# results printing
print(results_df)

ridge_lambda = ridge_model.named_steps["ridgecv"].alpha_
lasso_lambda = lasso_model.named_steps["lassocv"].alpha_

print("\nOptimal lambda values :")
print(f" Ridge lambda: {ridge_lambda:.6f}")
print(f" Lasso lambda: {lasso_lambda:.6f}")

# a smaller lambda means less penalties, adjusting standarization stength
# a larger one means coefficients shrink
# leading to a simpler modeling with less overfitting

# metrics for score_read
import numpy as np
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.pipeline import make_pipeline
from sklearn.linear_model import RidgeCV, LassoCV
from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score

data = base_df.copy()

# selecting variables, excluded identifiers
cols = [
    "score_read", "gender", "lunch", "class_type",
    "experience", "ethnicity", "school", "degree",
    "ladder", "t_ethnicity"
]
data = data[cols]

# since some values are empty, replace with "missing"
for col in data.columns:
    if data[col].dtype == "object":
        data[col] = data[col].fillna("Missing")
    else:
        data[col] = data[col].fillna(data[col].median())

# design matrix
y = data["score_read"]
X = pd.get_dummies(data.drop(columns="score_read"), drop_first=True)

# training data with 80/20
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, random_state=123
)

# ridge and lasso model
alphas = np.logspace(-2, 2, 50)  # broader grid for finer tuning

ridge_model = make_pipeline(
    StandardScaler(),
    RidgeCV(alphas=alphas, cv=10)
).fit(X_train, y_train)

lasso_model = make_pipeline(
    StandardScaler(),
    LassoCV(alphas=alphas, cv=10, random_state=123, max_iter=50000) #increased
).fit(X_train, y_train)

# Predictions
ridge_pred = ridge_model.predict(X_test)
lasso_pred = lasso_model.predict(X_test)

# function for metrics
def compute_metrics(y_true, y_pred, model_name, n_features):
    n = len(y_true)
    mse = mean_squared_error(y_true, y_pred)
    rmse = np.sqrt(mse)
    mae = mean_absolute_error(y_true, y_pred)
    mape = np.mean(np.abs((y_true - y_pred) / y_true)) * 100
    r2 = r2_score(y_true, y_pred)
    adj_r2 = 1 - ((1 - r2) * (n - 1)) / (n - n_features - 1)
    return {
        "Model": model_name,
        "MSE": mse,
        "RMSE": rmse,
        "MAE": mae,
        "MAPE": mape,
        "R2": r2,
        "Adjusted_R2": adj_r2
    }

ridge_p = np.sum(ridge_model.named_steps["ridgecv"].coef_ != 0)
lasso_p = np.sum(lasso_model.named_steps["lassocv"].coef_ != 0)

ridge_results = compute_metrics(y_test, ridge_pred, "Ridge", ridge_p)
lasso_results = compute_metrics(y_test, lasso_pred, "Lasso", lasso_p)

results_df = pd.DataFrame([ridge_results, lasso_results]).round(4)

# results printing
print(results_df)

ridge_lambda = ridge_model.named_steps["ridgecv"].alpha_
lasso_lambda = lasso_model.named_steps["lassocv"].alpha_

print("\nOptimal lambda values :")
print(f" Ridge lambda: {ridge_lambda:.6f}")
print(f" Lasso lambda: {lasso_lambda:.6f}")

# mild regularization needed, good structure

# Lasso Residual Plot for score_read
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.pipeline import make_pipeline
from sklearn.linear_model import LassoCV

data = base_df.copy()
cols = [
    "score_read", "gender", "lunch", "class_type",
    "experience", "ethnicity", "school", "degree",
    "ladder", "t_ethnicity"
]
data = data[cols]

# handle missing values
for col in data.columns:
    if data[col].dtype == "object":
        data[col] = data[col].fillna("Missing")
    else:
        data[col] = data[col].fillna(data[col].median())

# design matrix
y = data["score_read"]
X = pd.get_dummies(data.drop(columns="score_read"), drop_first=True)

# training split 80/20
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, random_state=123
)

# modeling lasso
alphas = np.logspace(-2, 2, 50)
lasso_model = make_pipeline(
    StandardScaler(),
    LassoCV(alphas=alphas, cv=10, random_state=123, max_iter=50000)
).fit(X_train, y_train)

lasso_pred = lasso_model.predict(X_test)
lasso_resid = y_test - lasso_pred

# plot
plt.figure(figsize=(9,6))
sns.scatterplot(x=lasso_pred, y=lasso_resid, alpha=0.6, color="orange")
plt.axhline(0, color="black", linestyle="--", linewidth=1)
plt.title("Residual Plot For Lasso Regression (score_read)", fontsize=14)
plt.xlabel("Fitted Values (Predicted score_read)")
plt.ylabel("Residuals (Actual − Predicted)")
plt.grid(alpha=0.3)
plt.show()

# Ridge Residual Plot for score_math
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.pipeline import make_pipeline
from sklearn.linear_model import RidgeCV

# Load and select
data = base_df.copy()
cols = [
    "score_math", "gender", "lunch", "class_type",
    "experience", "ethnicity", "school", "degree",
    "ladder", "t_ethnicity"
]
data = data[cols]

# Handle missing values
for col in data.columns:
    if data[col].dtype == "object":
        data[col] = data[col].fillna("Missing")
    else:
        data[col] = data[col].fillna(data[col].median())

# Design matrix
y = data["score_math"]
X = pd.get_dummies(data.drop(columns="score_math"), drop_first=True)

# Train-test split
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, random_state=123
)

# Ridge model (since it performed slightly better for math)
alphas = np.logspace(-5, 5, 300)
ridge_model = make_pipeline(
    StandardScaler(),
    RidgeCV(alphas=alphas, cv=10)
).fit(X_train, y_train)

# Predictions and residuals
ridge_pred = ridge_model.predict(X_test)
ridge_resid = y_test - ridge_pred

# Plot
plt.figure(figsize=(9,6))
sns.scatterplot(x=ridge_pred, y=ridge_resid, alpha=0.6, color="blue")
plt.axhline(0, color="black", linestyle="--", linewidth=1)
plt.title("Residual Plot For Ridge Regression (score_math)", fontsize=14)
plt.xlabel("Fitted Values (Predicted score_math)")
plt.ylabel("Residuals (Actual − Predicted)")
plt.grid(alpha=0.3)
plt.show()

# run score_math/read before to get driver result
ridge_coefs = pd.Series(
    ridge_model.named_steps["ridgecv"].coef_,
    index=X.columns
).sort_values(key=abs, ascending=False)

lasso_coefs = pd.Series(
    lasso_model.named_steps["lassocv"].coef_,
    index=X.columns
).sort_values(key=abs, ascending=False)

print("Top Ridge Coefficients:")
print(ridge_coefs.head(10))

print("\nTop Lasso Coefficients:")
print(lasso_coefs[lasso_coefs != 0].head(10))